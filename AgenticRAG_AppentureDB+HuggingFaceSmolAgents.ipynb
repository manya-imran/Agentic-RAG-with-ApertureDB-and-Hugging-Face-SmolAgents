{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Agentic RAG with ApertureDB and Hugging Face SmolAgents\n",
        "\n",
        "\n",
        "This implementation highlights the Agentic RAG implementation using ApertureDB data store, which is a graph-based multimodal database. Hugging Face SmolAgents will be employed for implementing a multi-agent LLM workflow.\\"
      ],
      "metadata": {
        "id": "6_fq3nPxaHLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Prepping the data\n",
        "\n",
        "For this implementation, we’ll be using an Arxiv structured complex dataset. Which will be first pre-processed into embeddings so that they can be stored in the ApertureDB vector database. The large dataset is divided into small chunks of data, out of which vector embeddings are generated using a model like sentence-transformer/all-MiniLM-L6-v2 from Hugging Face. These vector embeddings are then stored in apertureDB.\n"
      ],
      "metadata": {
        "id": "bq7T-gXIap85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4qGb196Z97p"
      },
      "outputs": [],
      "source": [
        "# arxiv complex structured data is used as dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "def get_arxiv_papers(query: str, max_results: int = 10) -> List[str]:\n",
        "    \"\"\"Fetch papers from arXiv and return their text content\"\"\"\n",
        "    client = arxiv.Client()\n",
        "    search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
        "    )\n",
        "\n",
        "    papers = []\n",
        "    for result in client.results(search):\n",
        "        papers.append(f\"Title: {result.title}\\nAbstract: {result.summary}\")\n",
        "\n",
        "    return papers\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 512) -> List[str]:\n",
        "    \"\"\"Split text into chunks of specified size\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "# Fetch and prepare data\n",
        "arxiv_query = \"graph embeddings\"\n",
        "papers = get_arxiv_papers(arxiv_query)\n",
        "chunks = []\n",
        "for paper in papers:\n",
        "    chunks.extend(chunk_text(paper))\n",
        "\n",
        "# Generate embeddings\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(chunks, show_progress_bar=True)\n",
        "\n",
        "print(f\"Generated {len(embeddings)} embeddings for {len(chunks)} chunks\")\n"
      ],
      "metadata": {
        "id": "9NCmzCeIatkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setting Up ApertureDB\n",
        "\n",
        "In this implementation, ApertureDB is used as a vector store that offers high performance and speedy retrievals, also supporting multimodal data. The vector based stores help in maintaining context as vectors while generating responses. ApertureDB also integrates seamlessly with AI pipelines like LangChain and Hugging Face. This is how you store vector embeddings in ApertureDB."
      ],
      "metadata": {
        "id": "vW7m4QygbB6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aperturedb\n",
        "from aperturedb.Connector import Connector\n",
        "import time\n",
        "class ApertureDBVectorStore:\n",
        "    def __init__(self, host: str = \"localhost\", port: int = 8000):\n",
        "        self.client = Connector({\"host\": host, \"port\": port})\n",
        "    def create_schema(self):\n",
        "        \"\"\"Create necessary schema in ApertureDB\"\"\"\n",
        "        schema = [ {\n",
        "                \"CreateProperty\": {\n",
        "                    \"name\": \"type\",\n",
        "                    \"type\": \"string\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"CreateProperty\": {\n",
        "                    \"name\": \"text\",\n",
        "                    \"type\": \"string\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"CreateVectorIndex\": {\n",
        "                    \"name\": \"default_embedding\",\n",
        "                    \"dimensions\": 384,\n",
        "                    \"metric\": \"cosine\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "        response, _ = self.client.query(schema)\n",
        "        return response\n",
        "    def store_embeddings(self, chunks: List[str], embeddings: np.ndarray):\n",
        "        \"\"\"Store text chunks with their embeddings in ApertureDB\"\"\"\n",
        "        records = []\n",
        "        for chunk, embedding in zip(chunks, embeddings):\n",
        "            records.append([\n",
        "                {\n",
        "                    \"AddObject\": {\n",
        "                        \"properties\": {\n",
        "                            \"type\": \"document\",\n",
        "                            \"text\": chunk\n",
        "                        },\n",
        "                        \"embedding\": {\n",
        "                            \"vector\": embedding.tolist(),\n",
        "                            \"name\": \"default_embedding\"\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            ])\n",
        "        batch_size = 50\n",
        "        for i in range(0, len(records), batch_size):\n",
        "            batch = records[i:i+batch_size]\n",
        "            response, blobs = self.client.query(batch)\n",
        "            print(f\"Inserted batch {i//batch_size + 1}, response: {response}\")\n",
        "            time.sleep(0.1)\n",
        "        return True\n",
        "# Initialize and populate ApertureDB\n",
        "aperture_db = ApertureDBVectorStore()\n",
        "aperture_db.create_schema()\n",
        "aperture_db.store_embeddings(chunks, embeddings)"
      ],
      "metadata": {
        "id": "p0BiaRIcbF5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Building the Agentic Workflow\n",
        "\n",
        "\n",
        "For the agentic flow, we proceed by defining agents using smolAgents in Hugging Face. These agents are responsible for query reformulation, iterative retrievals, and dynamic reranking. All of these are basically an iterative process of rewriting the initial query to get results and prioritizing the results of the iterations for the best result. Here’s how you define the agentic logic for query refinement and retrieval.\n"
      ],
      "metadata": {
        "id": "kzXDRCb4bKgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Any\n",
        "from langchain.vectorstores import VectorStore\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from smolagents import ToolCallingAgent, LiteLLMModel\n",
        "\n",
        "class ApertureDBRetriever:\n",
        "    def __init__(self, aperture_db: ApertureDBVectorStore, k: int = 5):\n",
        "        self.db = aperture_db\n",
        "        self.k = k\n",
        "\n",
        "    def similarity_search(self, query: str, k: int = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Perform similarity search in ApertureDB\"\"\"\n",
        "        k = k or self.k\n",
        "        query_vector = model.encode(query).tolist()\n",
        "\n",
        "        search_query = [\n",
        "            {\n",
        "                \"FindObject\": {\n",
        "                    \"with_vector\": {\n",
        "                        \"name\": \"default_embedding\",\n",
        "                        \"vector\": query_vector,\n",
        "                        \"k\": k\n",
        "                    },\n",
        "                    \"properties\": [\"text\"],\n",
        "                    \"results\": {\n",
        "                        \"list\": [\"text\"]\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        response, _ = self.db.client.query(search_query)\n",
        "        if not response or 'FindObject' not in response[0]:\n",
        "            return []\n",
        "\n",
        "        results = []\n",
        "        for i, item in enumerate(response[0]['FindObject']['entities']):\n",
        "            results.append({\n",
        "                \"content\": item['properties']['text'],\n",
        "                \"score\": item['vector_distance'],\n",
        "                \"index\": i\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "class AgenticRetriever:\n",
        "    def __init__(self, retriever: ApertureDBRetriever):\n",
        "        self.retriever = retriever\n",
        "\n",
        "    def __call__(self, query: str) -> str:\n",
        "        \"\"\"Retrieve relevant documents and format the output\"\"\"\n",
        "        retrieved_docs = self.retriever.similarity_search(query)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            return \"No relevant documents found.\"\n",
        "\n",
        "        output = \"Retrieved documents:\\n\"\n",
        "        for doc in retrieved_docs:\n",
        "            output += f\"\\n--- Document {doc['index'] + 1} (score: {doc['score']:.3f}) ---\\n\"\n",
        "            output += doc['content'][:500] + (\"...\" if len(doc['content']) > 500 else \"\")\n",
        "            output += \"\\n\"\n",
        "        return output\n",
        "aperture_retriever = ApertureDBRetriever(aperture_db)\n",
        "agentic_retriever = AgenticRetriever(aperture_retriever)"
      ],
      "metadata": {
        "id": "6sCK2hcQbWRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 4. Integrating with ApertureDB\n",
        "\n",
        "The last step is to connect the Hugging Face smolAgents defined to the ApertureDB containing the vector embeddings of the dataset RAG is working on. Here’s how you integrate the database with the Hugging Face Agentic AI pipeline for refined query and retrieving results.\n"
      ],
      "metadata": {
        "id": "L8vHT-YkbvFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables (for API keys)\n",
        "load_dotenv()\n",
        "\n",
        "def main():\n",
        "    # Initialize the agent with our retriever tool\n",
        "    model = LiteLLMModel(model_id=\"gpt-4-turbo\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    agent = ToolCallingAgent(\n",
        "        tools=[agentic_retriever],\n",
        "        model=model,\n",
        "        system_message=\"You are a helpful research assistant. Use the tools provided to retrieve relevant academic papers.\"\n",
        "    )\n",
        "\n",
        "    queries = [\n",
        "        \"why are graph embeddings used for context preservation\",\n",
        "        \"latest research on knowledge graph embeddings\",\n",
        "        \"comparison of different graph embedding techniques\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\n=== Query: {query} ===\")\n",
        "        response = agent.run(query)\n",
        "        print(\"\\nResponse:\")\n",
        "        print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "63GxL0sSbyMG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}